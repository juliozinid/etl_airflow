ğŸš€ Projeto: ETL Orquestrado com Apache Airflow

ğŸ“Œ DescriÃ§Ã£o

Este projeto implementa um pipeline de dados automatizado e escalÃ¡vel usando Apache Airflow, PostgreSQL e MongoDB. O objetivo Ã© consolidar informaÃ§Ãµes de acessos ao site (MongoDB) e pedidos de clientes (PostgreSQL) em um Data Warehouse (PostgreSQL) para anÃ¡lise.

ğŸ¯ Objetivos do Projeto

âœ… Extrair dados de MongoDB e PostgreSQLâœ… Transformar os dados, cruzando logs de acesso e pedidosâœ… Carregar os dados no Data Warehouse (PostgreSQL)âœ… Orquestrar o processo com Apache Airflowâœ… Criar um dashboard opcional para anÃ¡lise

ğŸ— Arquitetura do Projeto

etl_project/
â”‚â”€â”€ dags/                   # DAGs do Airflow
â”‚   â”œâ”€â”€ etl_pipeline.py     # DAG principal
â”‚â”€â”€ scripts/                # Scripts Python para ETL
â”‚   â”œâ”€â”€ extract_mongo.py    # ExtraÃ§Ã£o do MongoDB
â”‚   â”œâ”€â”€ extract_postgres.py # ExtraÃ§Ã£o do PostgreSQL
â”‚   â”œâ”€â”€ transform_data.py   # TransformaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ load_to_dwh.py      # Carregamento no Data Warehouse
â”‚â”€â”€ configs/                # ConfiguraÃ§Ãµes do projeto
â”‚   â”œâ”€â”€ airflow.cfg         # ConfiguraÃ§Ã£o do Airflow
â”‚   â”œâ”€â”€ db_config.json      # ConfiguraÃ§Ãµes de conexÃ£o com os bancos
â”‚â”€â”€ tests/                  # Testes automatizados
â”‚   â”œâ”€â”€ test_extract.py     # Testes de extraÃ§Ã£o
â”‚   â”œâ”€â”€ test_transform.py   # Testes de transformaÃ§Ã£o
â”‚   â”œâ”€â”€ test_load.py        # Testes de carga
â”‚â”€â”€ logs/                   # Logs do pipeline
â”‚â”€â”€ dashboards/             # Dashboard para anÃ¡lise de dados
â”‚   â”œâ”€â”€ dashboard.py        # AplicaÃ§Ã£o Streamlit para visualizaÃ§Ã£o
â”‚â”€â”€ docker/                 # ConfiguraÃ§Ã£o para rodar em Docker
â”‚   â”œâ”€â”€ Dockerfile          # Dockerfile para Airflow
â”‚   â”œâ”€â”€ docker-compose.yml  # Arquivo de configuraÃ§Ã£o do ambiente
â”‚â”€â”€ requirements.txt        # DependÃªncias do projeto
â”‚â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto

âš™ï¸ Tecnologias Utilizadas

ğŸ”¹ Apache Airflow - OrquestraÃ§Ã£o de workflowsğŸ”¹ PostgreSQL - Banco de dados relacionalğŸ”¹ MongoDB - Banco NoSQL para logs de acessoğŸ”¹ Python (pandas, psycopg2, pymongo) - Scripts de ETLğŸ”¹ Docker & Docker Compose - ContÃªineres para ambiente isoladoğŸ”¹ Streamlit (opcional) - Dashboard para visualizaÃ§Ã£o

ğŸš€ ConfiguraÃ§Ã£o do Ambiente

1ï¸âƒ£ Clonar o RepositÃ³rio

git clone https://github.com/seu-usuario/etl_airflow.git
cd etl_airflow

2ï¸âƒ£ Criar o Ambiente Docker

cd docker
docker-compose up -d --build

âœ… Isso iniciarÃ¡ os containers do Airflow, PostgreSQL e MongoDB.

3ï¸âƒ£ Acessar o Apache Airflow

ğŸ”— Acesse a interface do Airflow:ğŸ“Œ http://localhost:8080 (UsuÃ¡rio: admin | Senha: admin)

4ï¸âƒ£ Rodar o Pipeline ETL

No Airflow, ative o DAG etl_pipeline.

Aguarde a execuÃ§Ã£o automÃ¡tica ou acione manualmente.

Monitore os logs para verificar a execuÃ§Ã£o.

ğŸ›  Detalhes do Pipeline ETL

1ï¸âƒ£ ExtraÃ§Ã£o de Dados

ğŸ“Œ MongoDB: Logs de acesso ao siteğŸ“Œ PostgreSQL: Pedidos dos clientes

2ï¸âƒ£ TransformaÃ§Ã£o

ğŸ“Œ Cruzamento dos dados por user_idğŸ“Œ NormalizaÃ§Ã£o e limpeza dos registros

3ï¸âƒ£ Carga no Data Warehouse

ğŸ“Œ Salvando os dados transformados no PostgreSQL

ğŸ“Š Dashboard (Opcional)

Para visualizar os dados transformados, use Streamlit:

cd dashboards
streamlit run dashboard.py

Acesse http://localhost:8501 para ver os insights.

âœ… Testes Automatizados

Execute os testes para garantir a qualidade dos scripts:

pytest tests/

ğŸ“ Melhorias Futuras

ğŸš€ Adicionar monitoramento com Prometheus/GrafanağŸš€ Melhorar performance do ETL com batch insertsğŸš€ Implementar notificaÃ§Ãµes via Slack no Airflow

ğŸ“Œ ConclusÃ£o

Este projeto demonstra como criar um pipeline ETL robusto, escalÃ¡vel e automatizado com Apache Airflow e Docker. ğŸš€

Se tiver dÃºvidas ou sugestÃµes, abra uma issue ou entre em contato! ğŸ˜ƒ

