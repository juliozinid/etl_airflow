# ğŸš€ Projeto: ETL Orquestrado com Apache Airflow

## ğŸ“Œ DescriÃ§Ã£o

Este projeto implementa um **pipeline de dados automatizado e escalÃ¡vel** usando **Apache Airflow**, **PostgreSQL** e **MongoDB**. O objetivo Ã© consolidar informaÃ§Ãµes de **acessos ao site (MongoDB)** e **pedidos de clientes (PostgreSQL)** em um **Data Warehouse (PostgreSQL)** para anÃ¡lise.

## ğŸ¯ Objetivos do Projeto

âœ… Extrair dados de **MongoDB** e **PostgreSQL**\
âœ… Transformar os dados, cruzando logs de acesso e pedidos\
âœ… Carregar os dados no **Data Warehouse (PostgreSQL)**\
âœ… Orquestrar o processo com **Apache Airflow**\
âœ… Criar um **dashboard opcional** para anÃ¡lise

---

## ğŸ— Arquitetura do Projeto

```bash
etl_project/
â”‚â”€â”€ dags/                   # DAGs do Airflow
â”‚   â”œâ”€â”€ etl_pipeline.py     # DAG principal
â”‚â”€â”€ scripts/                # Scripts Python para ETL
â”‚   â”œâ”€â”€ extract_mongo.py    # ExtraÃ§Ã£o do MongoDB
â”‚   â”œâ”€â”€ extract_postgres.py # ExtraÃ§Ã£o do PostgreSQL
â”‚   â”œâ”€â”€ transform_data.py   # TransformaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ load_to_dwh.py      # Carregamento no Data Warehouse
â”‚â”€â”€ configs/                # ConfiguraÃ§Ãµes do projeto
â”‚   â”œâ”€â”€ airflow.cfg         # ConfiguraÃ§Ã£o do Airflow
â”‚   â”œâ”€â”€ db_config.json      # ConfiguraÃ§Ãµes de conexÃ£o com os bancos
â”‚â”€â”€ tests/                  # Testes automatizados
â”‚   â”œâ”€â”€ test_extract.py     # Testes de extraÃ§Ã£o
â”‚   â”œâ”€â”€ test_transform.py   # Testes de transformaÃ§Ã£o
â”‚   â”œâ”€â”€ test_load.py        # Testes de carga
â”‚â”€â”€ logs/                   # Logs do pipeline
â”‚â”€â”€ dashboards/             # Dashboard para anÃ¡lise de dados
â”‚   â”œâ”€â”€ dashboard.py        # AplicaÃ§Ã£o Streamlit para visualizaÃ§Ã£o
â”‚â”€â”€ docker/                 # ConfiguraÃ§Ã£o para rodar em Docker
â”‚   â”œâ”€â”€ Dockerfile          # Dockerfile para Airflow
â”‚   â”œâ”€â”€ docker-compose.yml  # Arquivo de configuraÃ§Ã£o do ambiente
â”‚â”€â”€ requirements.txt        # DependÃªncias do projeto
â”‚â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
```

---

## âš™ï¸ Tecnologias Utilizadas

ğŸ”¹ **Apache Airflow** - OrquestraÃ§Ã£o de workflows\
ğŸ”¹ **PostgreSQL** - Banco de dados relacional\
ğŸ”¹ **MongoDB** - Banco NoSQL para logs de acesso\
ğŸ”¹ **Python (pandas, psycopg2, pymongo)** - Scripts de ETL\
ğŸ”¹ **Docker & Docker Compose** - ContÃªineres para ambiente isolado\
ğŸ”¹ **Streamlit (opcional)** - Dashboard para visualizaÃ§Ã£o

---

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### **1ï¸âƒ£ Clonar o RepositÃ³rio**

```bash
git clone https://github.com/seu-usuario/etl_airflow.git
cd etl_airflow
```

### **2ï¸âƒ£ Criar o Ambiente Docker**

```bash
cd docker
docker-compose up -d --build
```

âœ… Isso iniciarÃ¡ os containers do **Airflow, PostgreSQL e MongoDB**.

### **3ï¸âƒ£ Acessar o Apache Airflow**

ğŸ”— Acesse a interface do Airflow:\
ğŸ“Œ `http://localhost:8080` (UsuÃ¡rio: `admin` | Senha: `admin`)

### **4ï¸âƒ£ Rodar o Pipeline ETL**

1. No Airflow, ative o DAG `etl_pipeline`.
2. Aguarde a execuÃ§Ã£o automÃ¡tica ou acione manualmente.
3. Monitore os logs para verificar a execuÃ§Ã£o.

---

## ğŸ›  Detalhes do Pipeline ETL

### **1ï¸âƒ£ ExtraÃ§Ã£o de Dados**

ğŸ“Œ **MongoDB:** Logs de acesso ao site\
ğŸ“Œ **PostgreSQL:** Pedidos dos clientes

### **2ï¸âƒ£ TransformaÃ§Ã£o**

ğŸ“Œ Cruzamento dos dados por `user_id`\
ğŸ“Œ NormalizaÃ§Ã£o e limpeza dos registros

### **3ï¸âƒ£ Carga no Data Warehouse**

ğŸ“Œ Salvando os dados transformados no PostgreSQL

---

## ğŸ“Š Dashboard (Opcional)

Para visualizar os dados transformados, use **Streamlit**:

```bash
cd dashboards
streamlit run dashboard.py
```

Acesse `http://localhost:8501` para ver os insights.

---

## âœ… Testes Automatizados

Execute os testes para garantir a qualidade dos scripts:

```bash
pytest tests/
```

---

## ğŸ“ Melhorias Futuras

ğŸš€ Adicionar **monitoramento com Prometheus/Grafana**\
ğŸš€ Melhorar performance do ETL com **batch inserts**\
ğŸš€ Implementar **notificaÃ§Ãµes via Slack** no Airflow

---

## ğŸ“Œ ConclusÃ£o

Este projeto demonstra como criar um **pipeline ETL robusto**, **escalÃ¡vel** e **automatizado** com Apache Airflow e Docker. ğŸš€

Se tiver dÃºvidas ou sugestÃµes, abra uma issue ou entre em contato! ğŸ˜ƒ

